---
title: 'Iris Data set: Kmean clustering case study'
author: "Dynasty"
date: "`r Sys.Date()`"
output:
  html_document:
    df: paged
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    number_sections: yes
    theme: readable
    highlight: haddock
    code_download: yes
  word_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



K-means clustering is a method of clustering data into a predetermined number of clusters. It is an unsupervised learning algorithm that tries to group data points in a way that minimizes the sum of the distances between the data points within each cluster.

The algorithm works as follows:

Specify the number of clusters, k.
Initialize k centroids randomly within the data.
Assign each data point to the nearest centroid.
Recompute the centroids as the mean of the data points assigned to each centroid.
Repeat steps 3 and 4 until the centroids stop moving or a maximum number of iterations is reached.
One limitation of K-means is that it assumes that the clusters are spherical and equally sized, which may not always be the case in real-world data. Additionally, the initial placement of the centroids can affect the final clustering, so it may be necessary to run the algorithm multiple times with different initialization to obtain the best results.

For this case study will be performing flower (iris) segmentation by  apply Kmean clustering. The objective is to understand the Kmean clustering.

Data attributes include: Sepal width and Length, petal width and length plus species.
Aim is to build a model that is able to predict flower species depending on attribute given.




# **Data Understanding**

Loading Important Libraries
```{r message = FALSE}
library(data.table)
library(dplyr)
library(tidyverse)
library(ggplot2)
```


 Load and view the data set 
```{r}
require("datasets")
```

```{r}
data("iris")
```

Viewing with the structure of the data set
```{r}
str(iris)
```


Statistical summary
```{r}
summary(iris)
```

Data set preview
```{r}
head(iris)
```


### **Pre-Processing the data set**

Since clustering is a type of Unsupervised Learning, we would not require Class Label(output) during execution of our algorithm. We will, therefore, remove Class Attribute “Species” and store it in another variable. We would then normalize the attributes between 0 and 1 using our own function.


```{r}
iris.new<- iris[, c(1, 2, 3, 4)]
iris.class<- iris[, "Species"]
head(iris.new)
```


Previewing the class column
```{r}
head(iris.class)
```

Normalizing the data set so that no particular attribute has more impact on clustering algorithm than others.

```{r}
normalize <- function(x){
  return ((x-min(x)) / (max(x)-min(x)))
}

iris.new$Sepal.Length<- normalize(iris.new$Sepal.Length)
iris.new$Sepal.Width<- normalize(iris.new$Sepal.Width)
iris.new$Petal.Length<- normalize(iris.new$Petal.Length)
iris.new$Petal.Width<- normalize(iris.new$Petal.Width)
head(iris.new)
```


Applying the K-means clustering algorithm with no. of centroids(k)=3
```{r}
result<- kmeans(iris.new,3) 
```


records preview
```{r}
result$size 
```

Getting the value of cluster center data point value(3 centers for k=3)
```{r}
result$centers 
```

 Getting the cluster vector that shows the cluster where each record falls
 
```{r}
result$cluster
```
 

Visualizing the  clustering results
```{r}
plot(iris[,1:2], col = result$cluster) 
```






```{r}
par(mfrow = c(2,2), mar = c(5,4,2,2))
```

Plotting to see how Sepal.Length and Sepal.Width data points have been distributed in clusters
```{r}
plot(iris.new[c(1,2)], col = result$cluster)
```




Plotting to see how Sepal.Length and Sepal.Width data points have been distributed originally as per "class" attribute in data set.
```{r}
plot(iris.new[c(1,2)], col = iris.class)
```





Plotting to see how Petal.Length and Petal.Width data points have been distributed in clusters

```{r}
plot(iris.new[c(1,2)], col = result$cluster)
plot(iris.new[c(3,4)], col = iris.class)
```


Result of table shows that Cluster 1 corresponds to Virginica, Cluster 2 corresponds to Versicolor and Cluster 3 to Setosa.

```{r}
table(result$cluster, iris.class)
```






In order to improve this accuracy further, will try different values of “k”. 

Applying the K-means clustering algorithm with no. of centroids(k)5
```{r}
result<- kmeans(iris.new,5) 
```


records preview
```{r}
result$size 
```

Getting the value of cluster center data point value(3 centers for k=5)
```{r}
result$centers 
```

 Getting the cluster vector that shows the cluster where each record falls
 
```{r}
result$cluster
```
 

Visualizing the  clustering results
```{r}
plot(iris[,1:2], col = result$cluster) 
```






```{r}
par(mfrow = c(2,2), mar = c(5,4,2,2))
```

Plotting to see how Sepal.Length and Sepal.Width data points have been distributed in clusters
```{r}
plot(iris.new[c(1,2)], col = result$cluster)
```




Plotting to see how Sepal.Length and Sepal.Width data points have been distributed originally as per "class" attribute in data set.
```{r}
plot(iris.new[c(1,2)], col = iris.class)
```





Plotting to see how Petal.Length and Petal.Width data points have been distributed in clusters

```{r}
plot(iris.new[c(1,2)], col = result$cluster)
plot(iris.new[c(3,4)], col = iris.class)
```


Result of table shows that Cluster 1 corresponds to Virginica, Cluster 2 corresponds to Versicolor and Cluster 3 to Setosa.

```{r}
table(result$cluster, iris.class)
```


There are several metrics that can be used to evaluate the success of K-means clustering:

Within-cluster sum of squares (WCSS): This measures the sum of the squared distances between the data points in a cluster and the centroid of the cluster. A small WCSS indicates that the data points in the cluster are close to the centroid, and the cluster is therefore "tight".

Silhouette score: This measures how well each data point is assigned to its own cluster. A high silhouette score indicates that the data points are well-separated from other clusters.

Calinski-Harabasz index: This measures the ratio of the sum of squared distances between the data points in a cluster and the centroid of the cluster, to the sum of squared distances between the data points and the centroid of the whole dataset. A high Calinski-Harabasz index indicates that the clusters are well-separated from each other.

Dunn index: This measures the ratio of the minimum distance between the centroids of two different clusters, to the maximum distance between the data points in the same cluster. A high Dunn index indicates that the clusters are well-separated from each other and the data points within each cluster are close to the centroid.

It is important to note that these metrics should not be used in isolation, as different metrics may be more or less relevant depending on the specific characteristics of the dataset. It is also important to consider the business context in which the clusters will be used, as the ultimate goal of clustering may be to solve a particular business problem.